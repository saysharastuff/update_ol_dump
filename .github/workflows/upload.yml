# GitHub Actions workflow: OpenLibrary dump sync
# - Download latest dumps (skip if up-to-date)
# - (Optional) Split large .txt.gz into chunks
# - Convert chunks to Parquet with controlled disk usage
# - Upload Parquet files and manifest to Hugging Face

name: Sync OpenLibrary Dumps
on:
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # daily at 03:00 UTC

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_REPO: sayshara/ol_dump
  MANIFEST: ol_sync_manifest.json
  SPLIT_DUMPS: 'true'

jobs:
  process-and-upload:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dump: [authors, editions, works]
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install pyarrow pandas huggingface_hub
      - name: Check and download dump
        run: |
          python3 .github/scripts/ol_sync.py check-download
        env:
          MANIFEST: ${{ env.MANIFEST }}
      - name: Split dump into chunks
        if: ${{ env.SPLIT_DUMPS == 'true' }}
        run: |
          python3 .github/scripts/ol_sync.py split --dump ${{ matrix.dump }}
      - name: Convert to Parquet
        run: |
          python3 .github/scripts/ol_sync.py convert --dump ${{ matrix.dump }}
      - name: Upload Parquet and manifest
        run: |
          python3 .github/scripts/ol_sync.py upload --parquet-files $(ls *.parquet)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ env.HF_REPO }}
          MANIFEST: ${{ env.MANIFEST }}