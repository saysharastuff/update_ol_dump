name: OpenLibrary Upload with General Stitching

on:
  workflow_dispatch:

jobs:
  download_and_split:
    name: Download + Split ${ matrix.file }
    runs-on: ubuntu-latest
    strategy:
      matrix:
        file:
          - ol_dump_authors_latest.txt.gz
          - ol_dump_editions_latest.txt.gz
          - ol_dump_works_latest.txt.gz
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install tqdm requests pandas pyarrow huggingface_hub

      - name: Download and Split/Convert
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          mkdir -p gz_cache
          python split_and_cache.py ${{ matrix.file }}

      - name: Upload GZ chunks as artifact (if split required)
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: chunks-${{ matrix.file }}
          path: chunks/${{ matrix.file.replace('.txt.gz', '') }}_chunk_*.txt.gz
          if-no-files-found: ignore
          retention-days: 7

  convert_and_upload:
    name: Convert Chunks to Parquet & Upload
    needs: download_and_split
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install pandas pyarrow tqdm huggingface_hub

      - name: Download all chunk artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Flatten chunk folder layout
        run: |
          mkdir -p flat_chunks
          find chunks -name '*.txt.gz' -exec mv {} flat_chunks/ \;

      - name: Convert and Upload all chunks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          for f in flat_chunks/*.txt.gz; do
            echo "ðŸ“¦ Processing $f"
            python fetch_and_upload_matrix.py "$f"
          done

  stitch_combined:
    name: Stitch Parquet Files
    needs: convert_and_upload
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dump_base:
          - ol_dump_authors_latest
          - ol_dump_editions_latest
          - ol_dump_works_latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install pyarrow tqdm huggingface_hub

      - name: Download all stitched parts
        uses: actions/download-artifact@v4
        with:
          path: stitched_parts

      - name: Flatten stitched parts
        run: |
          mkdir -p stitched_parts
          find stitched_parts -name '*.parquet' -exec mv {} stitched_parts/ \;

      - name: Stitch chunks for ${{ matrix.dump_base }}
        run: |
          mkdir -p stitched_full
          python stitch_parquet_files.py ${{ matrix.dump_base }}.txt.gz

      - name: Delete Parquet Chunks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python delete_parquet_chunks.py ${{ matrix.dump_base }}

      - name: Upload stitched Parquet file
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          huggingface-cli upload sayshara/ol_dump             stitched_full/${{ matrix.dump_base }}.full.parquet             stitched/${{ matrix.dump_base }}.full.parquet             --repo-type dataset --token "$HF_TOKEN"

  upload_manifest:
    name: Upload Sync Manifest
    needs: [convert_and_upload, stitch_combined]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install huggingface-hub CLI
        run: pip install huggingface_hub huggingface_hub[cli]

      - name: Upload ol_sync_manifest.json to HF Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          huggingface-cli upload sayshara/ol_dump             
          ol_sync_manifest.json metadata/ol_sync_manifest.json             
          --repo-type dataset --token "$HF_TOKEN"
