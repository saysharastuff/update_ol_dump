# GitHub Actions workflow: OpenLibrary dump sync
# - Download latest dumps (skip if up-to-date)
# - (Optional) Split large .txt.gz into chunks
# - Convert chunks to Parquet with controlled disk usage
# - Upload Parquet files and manifest to Hugging Face

name: Sync OpenLibrary Dumps
on:
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * *'  # daily at 03:00 UTC

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_REPO: sayshara/ol_dump
  MANIFEST: ol_sync_manifest.json
  SPLIT_DUMPS: 'true'

jobs:
  process-and-upload:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: pip install pyarrow pandas huggingface_hub
      - name: Process all dumps sequentially
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ env.HF_REPO }}
          MANIFEST: ${{ env.MANIFEST }}
        run: |
          for dump in authors editions works; do
            echo "Processing $dump"
            python3 .github/scripts/ol_sync.py check-download
            if [ "${{ env.SPLIT_DUMPS }}" = "true" ]; then
              python3 .github/scripts/ol_sync.py split --dump $dump
            fi
            python3 .github/scripts/ol_sync.py convert --dump $dump
            ls *.parquet > parquet_files.txt
            if [ -s parquet_files.txt ]; then
              python3 .github/scripts/ol_sync.py upload --parquet-files $(cat parquet_files.txt)
            fi
            rm -f *.parquet parquet_files.txt
          done